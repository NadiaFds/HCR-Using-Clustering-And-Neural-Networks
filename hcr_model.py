# -*- coding: utf-8 -*-
"""HCR Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XBaKd-BhfwEkbwUGtOzzztyM8T6eUXpk
"""

!pip install imutils

import numpy as np
from numpy import asarray
import pandas as pd
from keras.preprocessing.image import ImageDataGenerator
import os
import random 
import cv2
import imutils
import random
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelBinarizer
from keras.utils import np_utils
from keras.models import Sequential
from keras import optimizers
from sklearn.preprocessing import LabelBinarizer
from keras import backend as K
from keras.layers import Dense, Activation, Flatten, Dense,MaxPooling2D, Dropout
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization

"""# Calling K-Means Clustering Algorithm"""

# Commented out IPython magic to ensure Python compatibility.
# %run ./K-Means_Clustering_Algorithm.ipynb

"""# Calling Hierarchical Clustering Algorithm"""

# Commented out IPython magic to ensure Python compatibility.
# %run ./Hierarchical_Clustering_Algorithm.ipynb

"""# Calling Fuzzy C-Means Clustering Algorithm"""

# Commented out IPython magic to ensure Python compatibility.
# %run ./Fuzzy_C-Means_Clustering_Algorithm.ipynb
foo()

"""
#  CNN Model"""

dir = "Fuzzy C-Means Clustering/Train"
train_data = []
img_size = 28
#non_chars = ["#","$","&","@"]
for i in os.listdir(dir):
    #if i in non_chars:
        #continue
    #count = 0
    sub_directory = os.path.join(dir,i)
    for j in os.listdir(sub_directory):
        count+=1
        if count > 4000:
            break
        img = cv2.imread(os.path.join(sub_directory,j),0)
        print(sub_directory)

        img = cv2.resize(img,(img_size,img_size))
        train_data.append([img,i])

len(train_data)

val_dir = "Fuzzy C-Means Clustering/Validation"
val_data = []
img_size = 28
for i in os.listdir(val_dir):
    #if i in non_chars:
        #continue
    #count = 0
    sub_directory = os.path.join(val_dir,i)
    for j in os.listdir(sub_directory):
        count+=1
        if count > 1000:
            break
        img = cv2.imread(os.path.join(sub_directory,j),0)
        print(sub_directory)

        img = cv2.resize(img,(img_size,img_size))
        val_data.append([img,i])

len(val_data)

random.shuffle(train_data)
random.shuffle(val_data)

train_X = []
train_Y = []
for features,label in train_data:
    train_X.append(features)
    train_Y.append(label)
    
print(train_Y)

val_X = []
val_Y = []
for features,label in val_data:
    val_X.append(features)
    val_Y.append(label)
    
print(val_Y)

LB = LabelBinarizer()
train_Y = LB.fit_transform(train_Y)
val_Y = LB.fit_transform(val_Y)

train_X = np.array(train_X)/255.0
np_array = np.array(train_X)

train_X = np_array.reshape(-1,28,28,1)
train_Y = np.array(train_Y)

val_X = np.array(val_X)/255.0
np_array = np.array(val_X)

val_X = np_array.reshape(-1,28,28,1)
val_Y = np.array(val_Y)

print(train_X.shape,val_X.shape)

print(train_Y.shape,val_Y.shape)

model = Sequential()

model.add(Conv2D(32, (3, 3), padding = "same", activation='relu', input_shape=(28,28,1)))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))
 
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(24, activation='softmax'))

model.summary()

model.compile(loss='categorical_crossentropy', optimizer="adam",metrics=['accuracy'])

history = model.fit(train_X,train_Y, epochs=10, batch_size=10, validation_data = (val_X, val_Y),  verbose=1)

"""### Training Accuracy vs Validation Accuracy Graph"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Training Accuracy vs Validation Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""#### Training Loss vs Validation Loss Graph"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Training Loss vs Validation Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Loss', 'Validation Loss'], loc='upper left')
plt.show()

"""# Test"""

model.save(r'model.h5')

from keras.models import load_model
import cv2
import numpy as np

class_names = {0:'A',1:'B',2:'C',3:'D',4:'E',5:'F',6:'G',7:'H',8:'I',9:'J',10:'K',11:'L',12:'M',13:'N',14:'O',15:'P',16:'Q',17:'R',18:'S',19:'T',20:'U',21:'V',22:'W',23:'X', 24:'Y',25:'Z'}

model = load_model('model.h5')

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

img = cv2.imread('Fuzzy C-Means Clustering/Train/C/C7 (1).jpg')

plt.imshow(img)

img = cv2.resize(img,(28,28))
img = img[:,:,0]
img = np.reshape(img,[1,28,28,1])

classes = np.argmax(model.predict(img), axis = -1)

print(classes)

names = [class_names[i] for i in classes]

print(names)